{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "overfit_and_underfit.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL8UoOTmGGsL"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FklhSI0Gg9R"
      },
      "source": [
        "Before getting started, import the necessary packages:. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pZ8A2liqvgk"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "  \n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnAtAjqRYVXe"
      },
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs\n",
        "\n",
        "import tensorflow_docs as tfdocs\n",
        "import tensorflow_docs.modeling\n",
        "import tensorflow_docs.plots"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pnOU-ctX27Q"
      },
      "source": [
        "from  IPython import display\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pathlib\n",
        "import shutil\n",
        "import tempfile\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj6I4dvTtbUe"
      },
      "source": [
        "logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n",
        "shutil.rmtree(logdir, ignore_errors=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cweoTiruj8O"
      },
      "source": [
        "\n",
        "## The Higgs Dataset\n",
        "\n",
        "The goal of this tutorial is not to do particle physics, so don't dwell on the details of the dataset. It contains 11&#x202F;000&#x202F;000 examples, each with 28 features, and a binary class label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPjAvwb-6dFd"
      },
      "source": [
        "gz = tf.keras.utils.get_file('HIGGS.csv.gz', 'https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkiyUdaWIrww"
      },
      "source": [
        "FEATURES = 28"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFggl9gYKKRJ"
      },
      "source": [
        "The `tf.data.experimental.CsvDataset` class can be used to read csv records directly from a gzip file with no intermediate decompression step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHz4sLVQEVIU"
      },
      "source": [
        "ds = tf.data.experimental.CsvDataset( gz,[float(),]*(FEATURES+1), compression_type=\"GZIP\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzahEELTKlSV"
      },
      "source": [
        "That csv reader class returns a list of scalars for each record. The following function repacks that list of scalars into a (feature_vector, label) pair."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPD6ICDlF6Wf"
      },
      "source": [
        "def pack_row(*row):\n",
        "  label = row[0]\n",
        "  features = tf.stack(row[1:],1)\n",
        "  return features, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oa8tLuwLsbO"
      },
      "source": [
        "TensorFlow is most efficient when operating on large batches of data.\n",
        "\n",
        "So instead of repacking each row individually make a new `Dataset` that takes batches of 10000-examples, applies the `pack_row` function to each batch, and then splits the batches back up into individual records:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w-VHTwwGVoZ"
      },
      "source": [
        "packed_ds = ds.batch(10000).map(pack_row).unbatch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-49WbmsDxeI6"
      },
      "source": [
        "### EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUbxc5bxNSXV"
      },
      "source": [
        "Have a look at some of the records from this new `packed_ds`.\n",
        "\n",
        "The features are not perfectly normalized, but this is sufficient for this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk55Y0M-_iva"
      },
      "source": [
        "for features,label in packed_ds.batch(1000).take(1):\n",
        "  print(features[0])\n",
        "  plt.hist(features.numpy().flatten(), bins = 101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZmX1fdAxzcL"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "print(list(ds.take(2).as_numpy_iterator())[0])\n",
        "print(list(ds.take(2).as_numpy_iterator())[1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XGAS8TtV3jQ"
      },
      "source": [
        "featVals=[features.numpy()[0] for features,label in packed_ds.take(1000)]\n",
        "\n",
        "t=packed_ds.take(20000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHNHsAXtCSwC"
      },
      "source": [
        "#### Target Variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mSU-HZw_-2l"
      },
      "source": [
        "t = [label.numpy() for features,label in packed_ds.take(20000)]\n",
        "print(\"Values 1:\",sum(t))\n",
        "print(\"Values 0:\",sum(np.equal(t,0)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFK70_GnCWm_"
      },
      "source": [
        "#### Predictors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgsPcWzbDv5E"
      },
      "source": [
        "t = np.array([features.numpy() for features,label in packed_ds.take(20000)])\n",
        "print(\"Min Value:\",min(t.flatten()))\n",
        "print(\"Max Value:\",max(t.flatten()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfcXuv33Fvka"
      },
      "source": [
        "#for features,label in packed_ds.batch(1000).take(1):\n",
        "fig, axs = plt.subplots(6, 5,figsize=(15,20))\n",
        "fig.subplots_adjust(hspace = .2, wspace=.1)\n",
        "\n",
        "axs = axs.ravel()\n",
        "totMissing=0\n",
        "for f in  range([len(x) for x in ds.take(1)][0]-1):\n",
        "  print(f % 10,end='')\n",
        "  featVals=[features.numpy()[f] for features,label in packed_ds.take(20000)]\n",
        "  ax=axs[f]\n",
        "  ax.hist(featVals ,bins = 100)\n",
        "  ax.set_title('Feature: ' + str(f))\n",
        "  ax.text(x=0,y=1,s='min: '+str(round(min(featVals),2)), transform=ax.transAxes, fontsize=12,verticalalignment='top')\n",
        "  ax.text(x=0,y=0.93,s='max: '+str(round(max(featVals),2)), transform=ax.transAxes, fontsize=12,verticalalignment='top')\n",
        "  ax.text(x=0.55,y=1,s='mean: '+str(round(np.mean(featVals),2)), transform=ax.transAxes, fontsize=12,verticalalignment='top')\n",
        "  ax.text(x=0.55,y=0.93,s='sd: '+str(round(np.std(featVals),2)), transform=ax.transAxes, fontsize=12,verticalalignment='top')\n",
        "  totMissing += sum(np.isnan(featVals))\n",
        "  #if(f % 4 !=0): \n",
        "    #ax.get_yaxis().set_visible(False)\n",
        "plt.tight_layout()  \n",
        "print(\"\\nTotal Missing values: \", totMissing)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUmYzlxj3HJO"
      },
      "source": [
        "### Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICKZRY7gN-QM"
      },
      "source": [
        "To keep this tutorial relatively short use just the first 1000 samples for validation, and the next 10 000 for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmk49OqZIFZP"
      },
      "source": [
        "N_VALIDATION = int(1e3)\n",
        "N_TRAIN = int(1e4)\n",
        "BUFFER_SIZE = int(1e4)\n",
        "BATCH_SIZE = 500  # TODO: Note that the paper used 100, we can increase this to reduce training time.\n",
        "STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP3M9DmvON32"
      },
      "source": [
        "The `Dataset.skip` and `Dataset.take` methods make this easy.\n",
        "\n",
        "At the same time, use the `Dataset.cache` method to ensure that the loader doesn't need to re-read the data form the file on each epoch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8H_ZzpBOOk-"
      },
      "source": [
        "validate_ds = packed_ds.take(N_VALIDATION).cache()\n",
        "train_ds = packed_ds.skip(N_VALIDATION).take(N_TRAIN).cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zAOqk2_Px7K"
      },
      "source": [
        "train_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PMliHoVO3OL"
      },
      "source": [
        "These datasets return individual examples. Use the `.batch` method to create batches of an appropriate size for training. Before batching also remember to `.shuffle` and `.repeat` the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7I4J355O223"
      },
      "source": [
        "validate_ds = validate_ds.batch(BATCH_SIZE)\n",
        "train_ds = train_ds.shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ReKHdC2EgVu"
      },
      "source": [
        "### Training procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNzkSkkXSP5l"
      },
      "source": [
        "Many models train better if you gradually reduce the learning rate during training. Use `optimizers.schedules` to reduce the learning rate over time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzVeXZJ8D-Zd"
      },
      "source": [
        "# https://www.tensorflow.org/tutorials/text/transformer\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "  def __call__(self, step):\n",
        "    ## Original Paper\n",
        "    # update_callbacks=pylearn2.training_algorithms.sgd.ExponentialDecay(\n",
        "    #                                     decay_factor=1.0000002, # Decreases by this factor every batch. (1/(1.000001^8000)^100 \n",
        "    #                                     min_lr=.000001\n",
        "    #                                     )\n",
        "    \n",
        "    # Implementation in TensorFlow\n",
        "    lr = tf.clip_by_value(0.05 / 1.0000002**step, clip_value_min=0.000001, clip_value_max=0.05)\n",
        "    return lr\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAY8ev64NCp6"
      },
      "source": [
        "tf.range(25, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXj15WO8EZ1q"
      },
      "source": [
        "temp_lr_schedule = CustomSchedule()\n",
        "plt.figure(figsize = (8,6))\n",
        "lrs = temp_lr_schedule(tf.range(40000000, dtype=tf.float32))\n",
        "plt.plot(lrs)\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-VxgEJBOa_N"
      },
      "source": [
        "lrs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwQp-ERhAD6F"
      },
      "source": [
        "def get_optimizer():\n",
        "  lr_schedule = CustomSchedule()\n",
        "  return tf.keras.optimizers.SGD(lr_schedule, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya7x7gr9UjU0"
      },
      "source": [
        "Each model in this tutorial will use the same training configuration. So set these up in a reusable way, starting with the list of callbacks.\n",
        "\n",
        "The training for this tutorial runs for many short epochs. To reduce the logging noise use the `tfdocs.EpochDots` which simply a `.` for each epoch and, and a full set of metrics every 100 epochs.\n",
        "\n",
        "Next include `callbacks.EarlyStopping` to avoid long and unnecessary training times. Note that this callback is set to monitor the `val_binary_crossentropy`, not the `val_loss`. This difference will be important later.\n",
        "\n",
        "Use `callbacks.TensorBoard` to generate TensorBoard logs for the training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSv8rfw_T85n"
      },
      "source": [
        "def get_callbacks(name):\n",
        "  return [\n",
        "    tfdocs.modeling.EpochDots(),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', min_delta=0.00001, patience=10),\n",
        "    tf.keras.callbacks.TensorBoard(logdir/name),\n",
        "  ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhctzKhBWVDD"
      },
      "source": [
        "Similarly each model will use the same `Model.compile` and `Model.fit` settings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRCGwU3YH5sT"
      },
      "source": [
        "def compile_and_fit(model, name, optimizer=None, max_epochs=10000):\n",
        "  if optimizer is None:\n",
        "    optimizer = get_optimizer()\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                # loss=tf.keras.metrics.AUC(),\n",
        "                metrics=[\n",
        "                  tf.keras.metrics.AUC(name='AUC'),\n",
        "                  tf.keras.losses.BinaryCrossentropy(from_logits=True, name='binary_crossentropy'),\n",
        "                  'accuracy'])\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  history = model.fit(\n",
        "    train_ds,\n",
        "    steps_per_epoch = STEPS_PER_EPOCH,\n",
        "    epochs=max_epochs,\n",
        "    validation_data=validate_ds,\n",
        "    callbacks=get_callbacks(name),\n",
        "    verbose=2)\n",
        "  return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRcoWzddIH4C"
      },
      "source": [
        "size_histories = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNUnRALzTFuE"
      },
      "source": [
        "### Model from Paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-J6r1RwTIiD"
      },
      "source": [
        "# https://www.tensorflow.org/api_docs/python/tf/keras/initializers/RandomNormal\n",
        "first_initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=0.1, seed=42)\n",
        "outer_initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=0.001, seed=42)\n",
        "other_initializers = tf.keras.initializers.RandomNormal(mean=0., stddev=0.05, seed=42)\n",
        "\n",
        "# Top Layer (https://www.quora.com/Are-the-top-layers-of-a-deep-neural-network-the-first-layers-or-the-last-layers)\n",
        "# Weight Decay: https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/\n",
        "weight_decay=0.00001\n",
        "paper_model = tf.keras.Sequential([\n",
        "    layers.Dense(300, activation='tanh', input_shape=(FEATURES,), kernel_initializer=first_initializer, kernel_regularizer=l2(weight_decay)),\n",
        "    layers.Dense(300, activation='tanh', kernel_initializer=other_initializers, kernel_regularizer=l2(weight_decay)),\n",
        "    layers.Dense(300, activation='tanh', kernel_initializer=other_initializers, kernel_regularizer=l2(weight_decay)),\n",
        "    layers.Dense(300, activation='tanh', kernel_initializer=other_initializers, kernel_regularizer=l2(weight_decay)),\n",
        "    layers.Dropout(0.5), # Top Hidden Layer\n",
        "    layers.Dense(1, activation='sigmoid', kernel_initializer=outer_initializer, kernel_regularizer=l2(weight_decay))\n",
        "    \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN0064SxTFZs"
      },
      "source": [
        "size_histories['paper'] = compile_and_fit(paper_model, 'sizes/paper')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agMT_i203Th7"
      },
      "source": [
        "size_histories['paper'].history['AUC'][0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45QB9DbxT4QT"
      },
      "source": [
        "plotter = tfdocs.plots.HistoryPlotter(metric = 'binary_crossentropy', smoothing_std=10)\n",
        "plotter.plot(size_histories)\n",
        "# plt.ylim([0.5, 0.7])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSkeG7D13Xak"
      },
      "source": [
        "plotter = tfdocs.plots.HistoryPlotter(metric = 'AUC', smoothing_std=10)\n",
        "plotter.plot(size_histories)\n",
        "# plt.ylim([0.5, 0.7])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy3CMUZpzH3d"
      },
      "source": [
        "### Plot the training and validation losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSlo1F4xHuuM"
      },
      "source": [
        "The solid lines show the training loss, and the dashed lines show the validation loss (remember: a lower validation loss indicates a better model)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLhL1AszdLfM"
      },
      "source": [
        "While building a larger model gives it more power, if this power is not constrained somehow it can easily overfit to the training set.\n",
        "\n",
        "In this example, typically, only the `\"Tiny\"` model manages to avoid overfitting altogether, and each of the larger models overfit the data more quickly. This becomes so severe for the `\"large\"` model that you need to switch the plot to a log-scale to really see what's happening.\n",
        "\n",
        "This is apparent if you plot and compare the validation metrics to the training metrics.\n",
        "\n",
        "* It's normal for there to be a small difference.\n",
        "* If both metrics are moving in the same direction, everything is fine.\n",
        "* If the validation metric begins to stagnate while the training metric continues to improve, you are probably close to overfitting.\n",
        "* If the validation metric is going in the wrong direction, the model is clearly overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XmKDtOWzOpk"
      },
      "source": [
        "plotter.plot(size_histories)\n",
        "a = plt.xscale('log')\n",
        "plt.xlim([5, max(plt.xlim())])\n",
        "plt.ylim([0.5, 0.7])\n",
        "plt.xlabel(\"Epochs [Log Scale]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UekcaQdmZxnW"
      },
      "source": [
        "Note: All the above training runs used the `callbacks.EarlyStopping` to end the training once it was clear the model was not making progress."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEQNKadHA0M3"
      },
      "source": [
        "### View in TensorBoard\n",
        "\n",
        "These models all wrote TensorBoard logs during training.\n",
        "\n",
        "To open an embedded  TensorBoard viewer inside a notebook, copy the following into a code-cell:\n",
        "\n",
        "```\n",
        "%tensorboard --logdir {logdir}/sizes\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjqx3bywDPjf"
      },
      "source": [
        "You can view the [results of a previous run](https://tensorboard.dev/experiment/vW7jmmF9TmKmy3rbheMQpw/#scalars&_smoothingWeight=0.97) of this notebook on [TensorBoard.dev](https://tensorboard.dev/).\n",
        "\n",
        "TensorBoard.dev is a managed experience for hosting, tracking, and sharing ML experiments with everyone.\n",
        "\n",
        "It's also included in an `<iframe>` for convenience:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX5fcgrADwym"
      },
      "source": [
        "display.IFrame(\n",
        "    src=\"https://tensorboard.dev/experiment/vW7jmmF9TmKmy3rbheMQpw/#scalars&_smoothingWeight=0.97\",\n",
        "    width=\"100%\", height=\"800px\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDQDBKYZBXF_"
      },
      "source": [
        "If you want to share TensorBoard results you can upload the logs to [TensorBoard.dev](https://tensorboard.dev/) by copying the following into a code-cell.\n",
        "\n",
        "Note: This step requires a Google account.\n",
        "\n",
        "```\n",
        "!tensorboard dev upload --logdir  {logdir}/sizes\n",
        "```\n",
        "\n",
        "Caution: This command does not terminate. It's designed to continuously upload the results of long-running experiments. Once your data is uploaded you need to stop it using the \"interrupt execution\" option in your notebook tool."
      ]
    }
  ]
}